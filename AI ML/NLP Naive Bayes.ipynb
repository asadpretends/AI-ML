{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2119fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#library for regular expretion\n",
    "import re\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6097ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"naive_bayes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42478013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Review Sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This book is good, very good.</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This book is bad, very bad.</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This novel is good.</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This novel is bad.</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This book is some good and some bad.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This novel is little good but little bad.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         User Review Sentence label  Unnamed: 2  Unnamed: 3  \\\n",
       "0              This book is good, very good.      +         NaN         NaN   \n",
       "1                 This book is bad, very bad.     +         NaN         NaN   \n",
       "2                        This novel is good.      +         NaN         NaN   \n",
       "3                         This novel is bad.      -         NaN         NaN   \n",
       "4       This book is some good and some bad.      0         NaN         NaN   \n",
       "5  This novel is little good but little bad.      0         NaN         NaN   \n",
       "\n",
       "   Unnamed: 4  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "5         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e1261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "data['label'] = le.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d116bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6\n",
      "1    6\n",
      "2    4\n",
      "3    4\n",
      "4    8\n",
      "5    8\n",
      "Name: User Review Sentence, dtype: int64\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "Words_count = data['User Review Sentence'].str.split().str.len()\n",
    "print(Words_count)\n",
    "lower_msg = data['User Review Sentence'].str.lower().str.split()\n",
    "print(len(lower_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e6b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(tokens):\n",
    "    alphanum =[]\n",
    "    for token in tokens:\n",
    "        new_s = ''.join(c for c in token if c.isalnum())\n",
    "        alphanum.append(new_s)\n",
    "    return alphanum\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b49536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lower_msg)):\n",
    "    lower_msg[i] = alpha(lower_msg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72dc28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   [this, book, is, good, very, good]\n",
      "1                     [this, book, is, bad, very, bad]\n",
      "2                              [this, novel, is, good]\n",
      "3                               [this, novel, is, bad]\n",
      "4         [this, book, is, some, good, and, some, bad]\n",
      "5    [this, novel, is, little, good, but, little, bad]\n",
      "Name: User Review Sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Message = lower_msg\n",
    "print(Message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6d6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence(tokens):\n",
    "    s = \"\"\n",
    "    for token in tokens:\n",
    "        s=s+\" \"+token\n",
    "    return s[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608ea6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lower_msg)):\n",
    "    lower_msg[i] = to_sentence(lower_msg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d2bdb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Message\n",
    "Y = data['label']\n",
    "Z = []\n",
    "stopwords = ['the','is','an','a' ,'here','their','threre']\n",
    "for i in X:\n",
    "    X = X.replace(stopwords, '', regex=True)\n",
    "    Z.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f627ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da7be4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "399bcd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da971bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vec.transform(X_train).toarray()\n",
    "X_test = vec.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f49f2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "542054b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model1 = MultinomialNB()\n",
    "model2 = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa047737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gaussian\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d7faaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multinomial\n",
    "model1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75c8638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bernouli\n",
    "model2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f0ee0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "Y_pred_1 = model1.predict(X_test)\n",
    "Y_pred_2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdcb0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian: 0.5\n",
      "Multinomial: 0.5\n",
      "Bernoulli: 0.5\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "accuracy1 =  accuracy_score(Y_test, Y_pred_1)\n",
    "accuracy2 =  accuracy_score(Y_test, Y_pred_2)\n",
    "\n",
    "print('Gaussian:',accuracy)\n",
    "print('Multinomial:',accuracy1)\n",
    "print('Bernoulli:',accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3830dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4473d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(Y_test, Y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "119318f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report1 = classification_report(Y_test, Y_pred_1)\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35933270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report2 = classification_report(Y_test, Y_pred_2)\n",
    "print(report2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac810bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86e712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
